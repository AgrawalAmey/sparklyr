---
title: "sparkworker: R Worker for Apache Spark"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

`sparkworker` provides support to execute arbitrary distributed r code, as any
other `sparklyr` extension, load `sparkworker`, `sparklyr` and connecto to
Apache Spark:

```{r}
library(sparkworker)
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.1.0")
iris_tbl <- sdf_copy_to(sc, iris)
```

To execute arbitrary functions use `spark_apply` as follows:

```{r}
spark_apply(iris_tbl, function(rows) {
  rows$Petal_Width <- rows$Petal_Width + 1
  rows
})
```

**Note:** Applying function closures is NYI.

Notice that `spark_log` shows `sparklyr` performing the following operations:

 1. The `Gateway` receives a request to execute custom `RDD` of type `WorkerRDD`.
 2. The `WorkerRDD` is evaluated on the worker node which initializes a new
    `sparklyr` backend tracked as `Worker` in the logs.
 3. The backend initializes an `RScript` process that connects back to the
    backend, retrieves data, performs the clossure and updates the result.

```{r}
spark_log(sc, filter = "sparklyr:")
```

Finally, we disconnect:

```{r}
spark_disconnect(sc)
```
