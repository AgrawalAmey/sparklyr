<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Distributing R Computations • sparklyr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/gallery.html">Gallery</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/guides-dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="../articles/guides-mllib.html">mllib</a>
    </li>
    <li>
      <a href="../articles/guides-h2o.html">H2O</a>
    </li>
    <li>
      <a href="../articles/guides-extensions.html">Extensions</a>
    </li>
    <li>
      <a href="../articles/guides-caching.html">Caching</a>
    </li>
    <li>
      <a href="../articles/guides-distributed-r.html">Distributed R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Deployment
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/deployment-overview.html">Overview</a>
    </li>
    <li>
      <a href="../articles/deployment-data-lakes.html">Data Lakes</a>
    </li>
    <li>
      <a href="../articles/deployment-cdh.html">Cloudera</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon.html">Amazon</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../reference/index.html">Functions</a>
    </li>
    <li>
      <a href="https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/sparklyr.pdf">Cheatsheet</a>
    </li>
    <li>
      <a href="../articles/reference-media.html">Media</a>
    </li>
    <li>
      <a href="../news/index.html">News</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/reticulate">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Distributing R Computations</h1>
            
          </div>

    
    
<div class="contents">
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p><strong>sparklyr</strong> provides support to run arbitrary R code at scale within your Spark Cluster through <code><a href="../reference/spark_apply.html">spark_apply()</a></code>. This is specially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor <a href="https://spark-packages.org/">Spark Packages</a>.</p>
<p>There are two main ways to make use of <code><a href="../reference/spark_apply.html">spark_apply()</a></code>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Computing over Rows</strong> enables you to transform each row with a custom function.</li>
<li>
<strong>Computing over Groups</strong> enables you to transform a custom group with a custom function.</li>
</ol>
<div id="computing-over-rows" class="section level3">
<h3 class="hasAnchor">
<a href="#computing-over-rows" class="anchor"></a>Computing over Rows</h3>
<p>Lets run the simplest example, the identify function over a list of numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)

sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)

<span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">5</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) e)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1782266bb32e3&gt; [?? x 1]
## # Database: spark_connection
##      id
##   &lt;dbl&gt;
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5</code></pre>
<p>Which returns what you would expect. We can take a look at the class of <code>e</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">class</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1782228c376f2&gt; [?? x 1]
## # Database: spark_connection
##           id
##        &lt;chr&gt;
## 1 data.frame</code></pre>
<p>Notice that a dataframe is provided which is indended to be used for row-by-row vectorized operations which, as it’s well known, is preffered within R.</p>
<p>Since Spark provides no guarantees on how the data is partitioned, specially after complex operations are perfomed over a dataset, it is possible and likely that the data will be partitioned in arbitraries ways so, agregation is not suitable for row-by-row operations. Notice that three entries when the data is explicitly partitioned by three:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">100</span>, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">nrow</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_17822476582d7&gt; [?? x 1]
## # Database: spark_connection
##      id
##   &lt;int&gt;
## 1    33
## 2    33
## 3    34</code></pre>
<p>That said, as long as the operations are performed row-by-row, there is significant functionality available. From applying custom function, filtering rows, duplicating rows, or modyfing columns as shown in the following example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris)

iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) e[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>])</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_17822f531159&gt; [?? x 3]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;
##  1          5.1         3.5          1.4
##  2          4.9         3.0          1.4
##  3          4.7         3.2          1.3
##  4          4.6         3.1          1.5
##  5          5.0         3.6          1.4
##  6          5.4         3.9          1.7
##  7          4.6         3.4          1.4
##  8          5.0         3.4          1.5
##  9          4.4         2.9          1.4
## 10          4.9         3.1          1.5
## # ... with 140 more rows</code></pre>
<p><code><a href="../reference/spark_apply.html">spark_apply()</a></code> defaults to use the column names from the original Spark dataframe; therefore, if a new column is added, or a rename is needed, the <code>names</code> parameter should be used:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">cbind</span>(<span class="fl">3.1416</span>, e), <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"Pi"</span>, <span class="kw">colnames</span>(iris)))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_17822518b86e3&gt; [?? x 6]
## # Database: spark_connection
##        Pi Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##     &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1 3.1416          5.1         3.5          1.4         0.2  setosa
##  2 3.1416          4.9         3.0          1.4         0.2  setosa
##  3 3.1416          4.7         3.2          1.3         0.2  setosa
##  4 3.1416          4.6         3.1          1.5         0.2  setosa
##  5 3.1416          5.0         3.6          1.4         0.2  setosa
##  6 3.1416          5.4         3.9          1.7         0.4  setosa
##  7 3.1416          4.6         3.4          1.4         0.3  setosa
##  8 3.1416          5.0         3.4          1.5         0.2  setosa
##  9 3.1416          4.4         2.9          1.4         0.2  setosa
## 10 3.1416          4.9         3.1          1.5         0.1  setosa
## # ... with 140 more rows</code></pre>
</div>
<div id="computing-over-groups" class="section level3">
<h3 class="hasAnchor">
<a href="#computing-over-groups" class="anchor"></a>Computing over Groups</h3>
<p>While working over rows can be really useful, it certainly restricts other use cases. For instance, suppose that you have a very large dataset that requires linear regression to be computed over several subgroups. To solve this, you can make use of the <code>group_by</code> parameter as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">nrow</span>(e), <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_178225d655561&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50</code></pre>
<p>This will force Spark to provide all the rows for that particular group to the function closure. To compute the intercept over this group, we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(
    <span class="cf">function</span>(e) <span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e)<span class="op">$</span>coefficients[[<span class="st">"(Intercept)"</span>]],
    <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_178225d1c5569&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;dbl&gt;
## 1 versicolor     1.781275
## 2  virginica     4.240653
## 3     setosa     1.327563</code></pre>
</div>
</div>
<div id="distributing-packages" class="section level2">
<h2 class="hasAnchor">
<a href="#distributing-packages" class="anchor"></a>Distributing Packages</h2>
<p>One of the most powerful features to use with <code><a href="../reference/spark_apply.html">spark_apply()</a></code> is the use of any package the R community provides. To use packages, install them before running <code><a href="../reference/spark-connections.html">spark_connect()</a></code>.</p>
<p>For instance, we can use the <a href="https://cran.r-project.org/package=broom">broom</a> package to create a tidy dataframe out of the linear regression as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy</a></span>(<span class="kw">lm</span>(Petal_Width <span class="op">~</span><span class="st"> </span>Petal_Length, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"term"</span>, <span class="st">"estimate"</span>, <span class="st">"std.error"</span>, <span class="st">"statistic"</span>, <span class="st">"p.value"</span>),
  <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_17822150b92c5&gt; [?? x 6]
## # Database: spark_connection
##      Species         term    estimate  std.error  statistic      p.value
##        &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01
## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11
## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03
## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02
## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01
## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02</code></pre>
<p><code>spark_apply</code> supports packages by copying the contents of your local <code>.libPaths()</code> into each worker node making use of <code>SparkConf.addFile()</code>. It’s not uncommon for R libraries to use hundreds of megabytes of disk space; which means that all this data will be copied once to each worker node and be persisted in each node while the <code>spark_connect</code> connection remains open. However, Spark is a cluster designed for big data processing and can efficiently copying much more data between nodes. This makes the distribution of R packages a relatively small one-time tax usually completed within a couple seconds in most clusters.</p>
<p>Notice that the <code>packages</code> parameter takes no effect under <code>master="local"</code> since packages already exists in the local machine.</p>
</div>
<div id="handling-errors" class="section level2">
<h2 class="hasAnchor">
<a href="#handling-errors" class="anchor"></a>Handling Errors</h2>
<p>While working with distributed systems, troubleshooting can be less straightforward than expected. For instance, the following code will cause the distributed execution of the function to fail and produce the following error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(iris_tbl, <span class="cf">function</span>(e) <span class="kw">stop</span>(<span class="st">"Make this fail"</span>))</code></pre></div>
<pre><code> Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details</code></pre>
<p>When getting this <code>check worker logs for details</code> message, it indicates that an error triggered while executing this function in a different computing node. While running in local mode, <code>sparklyr</code> will retrieve the worker logs that will look like:</p>
<pre><code>---- Output Log ----
(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)
17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows 
17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure 
17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail </code></pre>
<p>Which points out the real failure being <code>ERROR sparklyr: RScript (4190) Make this fail</code> as you would expect.</p>
<p>Is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs.</p>
</div>
<div id="requirements" class="section level2">
<h2 class="hasAnchor">
<a href="#requirements" class="anchor"></a>Requirements</h2>
<p>There are a few requirements worth listing out:</p>
<ul>
<li>The <strong>R Runtime</strong> is expected to be pre-installed in the cluster for <code>spark_apply</code> to function. Failure to install the cluster will trigger a <code>Cannot run program, no such file or directory</code> error while attempting to use <code><a href="../reference/spark_apply.html">spark_apply()</a></code>, contact your cluster administrator to consider making the R runtime available throughout the entire cluster.</li>
<li>An <strong>Homogenius Cluster</strong> is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.</li>
</ul>
</div>
<div id="configuration" class="section level2">
<h2 class="hasAnchor">
<a href="#configuration" class="anchor"></a>Configuration</h2>
<p>The following table describes relevant parameters while making use of <code>spark_apply</code>.</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Value</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>spark.r.command</code></td>
<td>The path to the R binary. Useful to select from multiple R versions.</td>
</tr>
<tr class="even">
<td><code>sparklyr.worker.gateway.address</code></td>
<td>The gateway address to use under each worker node. Defaults to <code>sparklyr.gateway.address</code>.</td>
</tr>
<tr class="odd">
<td><code>sparklyr.worker.gateway.port</code></td>
<td>The gateway port to use under each worker node. Defaults to <code>sparklyr.gateway.port</code>.</td>
</tr>
</tbody>
</table>
<p>For example, one could make use of an specific R version by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark_config.html">spark_config</a></span>()
config[[<span class="st">"spark.r.command"</span>]] &lt;-<span class="st"> "&lt;path-to-r-version&gt;"</span>

sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>, <span class="dt">config =</span> config)
<span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) e)</code></pre></div>
</div>
<div id="limitations" class="section level2">
<h2 class="hasAnchor">
<a href="#limitations" class="anchor"></a>Limitations</h2>
<div id="closures" class="section level3">
<h3 class="hasAnchor">
<a href="#closures" class="anchor"></a>Closures</h3>
<p>Closures are serialized using <code>serialize</code>, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of <code>serialize</code> is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references <code>external_value</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">external_value &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(iris_tbl, <span class="cf">function</span>(e) e <span class="op">+</span><span class="st"> </span>external_value)</code></pre></div>
</div>
<div id="livy" class="section level3">
<h3 class="hasAnchor">
<a href="#livy" class="anchor"></a>Livy</h3>
<p>Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.</p>
</div>
<div id="computing-over-groups-1" class="section level3">
<h3 class="hasAnchor">
<a href="#computing-over-groups-1" class="anchor"></a>Computing over Groups</h3>
<p>While performing computations over groups, <code><a href="../reference/spark_apply.html">spark_apply()</a></code> will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use <code><a href="http://dplyr.tidyverse.org/reference/do.html">dplyr::do</a></code> which is currently optimized for large partitions.</p>
</div>
<div id="package-installation" class="section level3">
<h3 class="hasAnchor">
<a href="#package-installation" class="anchor"></a>Package Installation</h3>
<p>Since packages are copied only once for the duration of the <code><a href="../reference/spark-connections.html">spark_connect()</a></code> connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, <code><a href="../reference/spark-connections.html">spark_disconnect()</a></code> the connection, modify packages and reconnect.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#overview">Overview</a></li>
      <li><a href="#distributing-packages">Distributing Packages</a></li>
      <li><a href="#handling-errors">Handling Errors</a></li>
      <li><a href="#requirements">Requirements</a></li>
      <li><a href="#configuration">Configuration</a></li>
      <li><a href="#limitations">Limitations</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
