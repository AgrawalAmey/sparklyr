---
title: "Tuning Spark connections"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---



## Hadoop cluster

Using Spark and R inside a Hadoop based *Data Lake* is very common at companies.  Because Spark is a relatively new technology, there is currently no good way to manage user connections to the Spark service centrally. There are some caps and settings that can be applied, but for most cases there are configurations that the R user will need to customize.  

This fact puts the R user in an unusual position of individually deciding how much resources to consume from the cluster, so it is very important to work with the cluster's administration team or individual to ensure an optimized experience for all.

The [Running on YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) page in Spark's official website is the best place to start for configuration settings reference, please bookmark it.  Both, cluster administrators and users can benefit from this document.  If Spark is new to the company, the [YARN tunning](https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/) article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works. 

### Important points 

- **Executors are not servers** - In Spark, a server can have multiple **Executors**.  Because of how Spark works, it is better to requests multiple smaller executors than a few large executors, the [YARN tunning](https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/) article mentioned above does a great job expanding on this point.

- **Spark configuration properties passed by R are just requests** - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session. Some limits are placed by Spark and some are placed by YARN. 

- **The cluster overrides 'silently'** -  Many times, no errors are returned when more than the allowed resources are requested, or if an attempt is made to change a setting fixed by the cluster. 

- **Requesting more memory or CPUs for Executors than allowed will return an error** - This is one of the exceptions to the cluster's 'silent' overrides.  It will return a message similar to this:
```
    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'
```
**A cluster's administrator** is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster's web UI.  Otherwise, changes to those settings are done directly in the *yarn-default.xml* file.


### Sample code

The numbers used in this example should be adjusted to you specific environment.  The code below will request 5 executors, and 300 megabytes of memory and 2 cores for each executor.  

```r
library(sparklyr)

conf <- spark_config()

conf$spark.executor.memory <- "300M"
conf$spark.executor.cores <- 2
conf$spark.executor.instances <- 5
conf$spark.dynamicAllocation.enabled <- "false"

sc <- spark_connect(master = "yarn-client", 
                    spark_home = "/usr/lib/spark/",
                    version = "1.6.0",
                    config = conf)
```

### Code breakdown

R  | Description
----------------------- | -------------------------------------------
`conf <- spark_config()`| Initialize a variable with the contents form `spark_config()`
`conf$spark.executor.memory <- "300M"` | Executor memory is capped by YARN
`conf$spark.executor.cores <- 2`  | 
`conf$spark.executor.instances <- 5` | Less instances may be allocated based on limits set by the cluster.  In this case, the limit is set at 3, so that is the most executors the session gets
`conf$spark.dynamicAllocation.enabled <- "false"` | Disabling Dynamic Allocation allows the `spark.executor.instances` to be used
`sc <- spark_connect(master = "yarn-client",` | Use `yarn-client` as the master
`                    spark_home = "/usr/lib/spark/",` | Point `spark_home` to the cluster's Spark folder
`                    version = "1.6.2",` | The version of Spark is restricted to what is available in the cluster 
`                    config = conf)` | Pass the configuration variable to the `config` argument

![](images/deployment/connections/yarn-client.jpg)

### Kerberos

There are two options to access a "kerberized" data lake:

- Use *kinit* to get and cache the ticket. After *kinit* is installed and configured, it can then be used in R via a `system()` call:p
```r
system("echo '<password>' | kinit <username>")
```
For more information visit this site: [Apache - Authenticate with kinit](http://directory.apache.org/apacheds/kerberos-ug/4.1-authenticate-kinit.html)

- A preferred option may be to use the out-of-the-box integration with Kerberos that the commercial offering of [RStudio Server](https://www.rstudio.com/products/rstudio-server-pro/) offers.

## Stand Alone cluster

[pending]

## Local 

`spakrlyr` enables the use of Spark locally in a laptop or desktop.  

### Sample code

```r

conf <- spark_config()

conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$spark.memory.fraction <- 0.9

sc <- spark_connect(master = "local", 
                    version = "2.1.0",
                    config = conf)
```

### Code breakdown

R  | Description
----------------------- | -------------------------------------------
`conf <- spark_config()`| Initialize a variable with the contents form `spark_config()`
`conf$sparklyr.cores.local  <- 4` | If not passed, it will default to the number of all cores in the computer
`conf$sparklyr.shell.driver-memory <- "16G"`| The session will have one executor, the driver.  This means that we can request as much memory as the computer can spare.
`conf$spark.memory.fraction <- 0.9` | Sets the percentage of the memory allocated for the data analysis.  It defaults to *0.4* or 40% of what is requested.  The value used in this instance will attempt to set the actual `driver-memory` to 13.5G, meaning 90% of 16.
`sc <- spark_connect(master = "local",`                           |  
`version = "2.1.0",   `                              | The default version will be set by `spark_install()`, but it can be overriden by passing `version`
`config = conf)` | Pass the configuration variable to the `config` argument


![](images/deployment/connections/local.jpg)

