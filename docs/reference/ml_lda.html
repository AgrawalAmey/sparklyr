<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Spark ML -- Latent Dirichlet Allocation — ml_lda â€¢ sparklyr</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/deployment-amazon-ec2.html">Spark Standalone Deployment in AWS</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon-emr.html">Using sparklyr with an Apache Spark cluster</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon.html">Deployments to Amazon</a>
    </li>
    <li>
      <a href="../articles/deployment-cdh.html">Using sparklyr with an Apache Spark cluster</a>
    </li>
    <li>
      <a href="../articles/deployment-connections.html">Configuring Spark Connections</a>
    </li>
    <li>
      <a href="../articles/deployment-data-lakes.html">Data Science using a Data Lake</a>
    </li>
    <li>
      <a href="../articles/deployment-overview.html">Deployment and Configuration</a>
    </li>
    <li>
      <a href="../articles/gallery.html">Analyzing Data with sparklyr</a>
    </li>
    <li>
      <a href="../articles/guides-caching.html">Understanding Spark Caching</a>
    </li>
    <li>
      <a href="../articles/guides-distributed-r.html">Distributing R Computations</a>
    </li>
    <li>
      <a href="../articles/guides-dplyr.html">Manipulating Data with dplyr</a>
    </li>
    <li>
      <a href="../articles/guides-extensions.html">Creating Extensions for sparklyr</a>
    </li>
    <li>
      <a href="../articles/guides-h2o.html">Sparkling Water (H2O) Machine Learning</a>
    </li>
    <li>
      <a href="../articles/guides-mllib.html">Spark Machine Learning Library (MLlib)</a>
    </li>
    <li>
      <a href="../articles/guides-textmining.html">Text mining with Spark &amp; sparklyr</a>
    </li>
    <li>
      <a href="../articles/reference-media.html">Media</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Spark ML -- Latent Dirichlet Allocation</h1>
    </div>

    
    <p>Latent Dirichlet Allocation (LDA), a topic model designed for text documents.</p>
    

    <pre class="usage"><span class='fu'>ml_lda</span>(<span class='no'>x</span>, <span class='kw'>k</span> <span class='kw'>=</span> <span class='fl'>10L</span>, <span class='kw'>max_iter</span> <span class='kw'>=</span> <span class='fl'>20L</span>, <span class='kw'>doc_concentration</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>topic_concentration</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>subsampling_rate</span> <span class='kw'>=</span> <span class='fl'>0.05</span>,
  <span class='kw'>optimizer</span> <span class='kw'>=</span> <span class='st'>"online"</span>, <span class='kw'>checkpoint_interval</span> <span class='kw'>=</span> <span class='fl'>10L</span>,
  <span class='kw'>keep_last_checkpoint</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>learning_decay</span> <span class='kw'>=</span> <span class='fl'>0.51</span>,
  <span class='kw'>learning_offset</span> <span class='kw'>=</span> <span class='fl'>1024</span>, <span class='kw'>optimize_doc_concentration</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>seed</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>features_col</span> <span class='kw'>=</span> <span class='st'>"features"</span>, <span class='kw'>topic_distribution_col</span> <span class='kw'>=</span> <span class='st'>"topicDistribution"</span>,
  <span class='kw'>uid</span> <span class='kw'>=</span> <span class='fu'><a href='random_string.html'>random_string</a></span>(<span class='st'>"lda_"</span>), <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p></td>
    </tr>
    <tr>
      <th>k</th>
      <td><p>The number of clusters to create</p></td>
    </tr>
    <tr>
      <th>max_iter</th>
      <td><p>The maximum number of iterations to use.</p></td>
    </tr>
    <tr>
      <th>doc_concentration</th>
      <td><p>Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). See details.</p></td>
    </tr>
    <tr>
      <th>topic_concentration</th>
      <td><p>Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms.</p></td>
    </tr>
    <tr>
      <th>subsampling_rate</th>
      <td><p>(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with <code>max_iter</code> so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.</p></td>
    </tr>
    <tr>
      <th>optimizer</th>
      <td><p>Optimizer or inference algorithm used to estimate the LDA model. Supported: "online" for Online Variational Bayes (default) and "em" for Expectation-Maximization.</p></td>
    </tr>
    <tr>
      <th>checkpoint_interval</th>
      <td><p>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</p></td>
    </tr>
    <tr>
      <th>keep_last_checkpoint</th>
      <td><p>(For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If <code>FALSE</code>, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless.</p></td>
    </tr>
    <tr>
      <th>learning_decay</th>
      <td><p>(For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called "kappa" in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.</p></td>
    </tr>
    <tr>
      <th>learning_offset</th>
      <td><p>(For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called "tau0" in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.</p></td>
    </tr>
    <tr>
      <th>optimize_doc_concentration</th>
      <td><p>(For Online optimizer only) Indicates whether the <code>doc_concentration</code> (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: <code>FALSE</code></p></td>
    </tr>
    <tr>
      <th>seed</th>
      <td><p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p></td>
    </tr>
    <tr>
      <th>features_col</th>
      <td><p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</p></td>
    </tr>
    <tr>
      <th>topic_distribution_col</th>
      <td><p>Output column with estimates of the topic mixture distribution for each document (often called "theta" in the literature). Returns a vector of zeros for an empty document.</p></td>
    </tr>
    <tr>
      <th>uid</th>
      <td><p>A character string used to uniquely identify the ML estimator.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>Optional arguments; currently unused.</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>The object returned depends on the class of <code>x</code>.</p><ul>
<li><p><code>spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. The object contains a pointer to
  a Spark <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.</p></li>
<li><p><code>ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the clustering estimator appended to the pipline.</p></li>
<li><p><code>tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a clustering model.</p></li>
<li><p><code>tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the estimator. The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>. This signature does not apply to <code>ml_lda()</code>.</p></li>
</ul>

    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>Terminology for LDA:</p><ul>
<li><p>"term" = "word": an element of the vocabulary</p></li>
<li><p>"token": instance of a term appearing in a document</p></li>
<li><p>"topic": multinomial distribution over terms representing some concept</p></li>
<li><p>"document": one piece of text, corresponding to one row in the input data</p></li>
</ul>
    <p>Original LDA paper (journal version): Blei, Ng, and Jordan. "Latent Dirichlet Allocation." JMLR, 2003.</p>
<p>Input data (<code>features_col</code>): LDA is given a collection of documents as input data, via the <code>features_col</code> parameter. Each document is specified as a Vector of length <code>vocab_size</code>, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code> and <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code> can be useful for converting text to word count vectors</p>
    
    <h2 class="hasAnchor" id="parameter-details"><a class="anchor" href="#parameter-details"></a>Parameter details</h2>

    
    <h3><code>doc_concentration</code></h3>
    This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). If not set by the user, then <code>doc_concentration</code> is set automatically. If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. Otherwise, the <code>doc_concentration</code> vector must be length k. (default = automatic)
    Optimizer-specific parameter settings:
    EM
    <ul>
<li><p>Currently only supports symmetric distributions, so all values in the vector should be the same.</p></li>
<li><p>Values should be greater than 1.0</p></li>
<li><p>default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. (2009), who recommend a +1 adjustment for EM.</p></li>
</ul>
    Online
    <ul>
<li><p>Values should be greater than or equal to 0</p></li>
<li><p>default = uniformly (1.0 / k), following the implementation from <a href='https://github.com/Blei-Lab/onlineldavb'>here</a></p></li>
</ul>
    <h3><code>topic_concentration</code></h3>
    This is the parameter to a symmetric Dirichlet distribution.
    Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009.
    If not set by the user, then <code>topic_concentration</code> is set automatically. (default = automatic)
    Optimizer-specific parameter settings:
    EM
    <ul>
<li><p>Value should be greater than 1.0</p></li>
<li><p>default = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.</p></li>
</ul>
    Online
    <ul>
<li><p>Value should be greater than or equal to 0</p></li>
<li><p>default = (1.0 / k), following the implementation from <a href='https://github.com/Blei-Lab/onlineldavb'>here</a>.</p></li>
</ul>
    <h3><code>topic_distribution_col</code></h3>
    This uses a variational approximation following Hoffman et al. (2010), where the approximate distribution is called "gamma." Technically, this method returns this approximation "gamma" for each document.
    
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <p>See <a href='http://spark.apache.org/docs/latest/ml-clustering.html'>http://spark.apache.org/docs/latest/ml-clustering.html</a> for
  more information on the set of clustering algorithms.</p>
<p>Other ml clustering algorithms: <code><a href='ml_bisecting_kmeans.html'>ml_bisecting_kmeans</a></code>,
  <code><a href='ml_gaussian_mixture.html'>ml_gaussian_mixture</a></code>,
  <code><a href='ml_kmeans.html'>ml_kmeans</a></code></p>
    

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>

      <li><a href="#parameter-details">Parameter details</a></li>

      <li><a href="#see-also">See also</a></li>
          </ul>

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Kuo, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
