% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_connection.R
\name{spark-api}
\alias{hive_context}
\alias{java_context}
\alias{spark-api}
\alias{spark_context}
\alias{spark_session}
\title{Access the Spark API}
\usage{
spark_context(sc)

java_context(sc)

hive_context(sc)

spark_session(sc)
}
\arguments{
\item{sc}{A \code{spark_connection}.}
}
\description{
Access the commonly-used Spark objects associated with a Spark instance.
These objects provide access to different facets of the Spark API.
}
\details{
The \href{http://spark.apache.org/docs/latest/api/scala/#package}{Scala API documentation}
is useful for discovering what methods are available for each of the Spark
APIs.
}
\section{Hive Context}{


With Spark >= 2.0.0, the \code{HiveContext} class has been effectively
superceded by the \code{SparkSession} class, and so \code{hive_context}
will return a \code{SparkSession} object instead. (Note that the
\code{SparkSession} will have been constructed with Hive support when
available.)
}

\section{Spark Session}{


This object is only available since Spark 2.0.0, and provides an interface
that unifies the Spark SQL context + Hive context into a single object, and
its use is recommended over the older APIs for code targetting Spark 2.0.0
and above.
}

