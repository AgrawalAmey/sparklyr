% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_clustering_lda.R
\name{ml_lda}
\alias{ml_lda}
\title{Spark ML -- Latent Dirichlet Allocation}
\usage{
ml_lda(x, k = 10L, max_iter = 20L, doc_concentration = NULL,
  topic_concentration = NULL, subsampling_rate = 0.05,
  optimizer = "online", checkpoint_interval = 10L,
  keep_last_checkpoint = TRUE, learning_decay = 0.51,
  learning_offset = 1024, optimize_doc_concentration = TRUE, seed = NULL,
  features_col = "features", topic_distribution_col = "topicDistribution",
  uid = random_string("lda_"), ...)
}
\description{
Latent Dirichlet Allocation (LDA), a topic model designed for text documents.
}
\details{
Terminology:
\itemize{
  \item "term" = "word": an element of the vocabulary
  \item "token": instance of a term appearing in a document
  \item "topic": multinomial distribution over terms representing some concept
  \item "document": one piece of text, corresponding to one row in the input data
}

Original LDA paper (journal version): Blei, Ng, and Jordan. "Latent Dirichlet Allocation." JMLR, 2003.

Input data (\code{features_col}): LDA is given a collection of documents as input data, via the \code{features_col} parameter. Each document is specified as a Vector of length \code{vocab_size}, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as \code{\link{ft_tokenizer}} and \code{\link{ft_count_vectorizer}} can be useful for converting text to word count vectors
}
